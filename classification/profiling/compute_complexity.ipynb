{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "from torchvision import models\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 16, 512, 512]              16\n",
      "            Conv2d-2         [-1, 16, 514, 514]              16\n",
      "            Conv2d-3         [-1, 16, 514, 514]              16\n",
      "     AttentionConv-4         [-1, 16, 512, 512]               0\n",
      "       BatchNorm2d-5         [-1, 16, 512, 512]              32\n",
      "              ReLU-6         [-1, 16, 512, 512]               0\n",
      "            Conv2d-7         [-1, 16, 512, 512]             256\n",
      "            Conv2d-8         [-1, 16, 514, 514]             256\n",
      "            Conv2d-9         [-1, 16, 514, 514]             256\n",
      "    AttentionConv-10         [-1, 16, 512, 512]               0\n",
      "      BatchNorm2d-11         [-1, 16, 512, 512]              32\n",
      "             ReLU-12         [-1, 16, 512, 512]               0\n",
      "        MaxPool2d-13         [-1, 16, 256, 256]               0\n",
      "           Conv2d-14         [-1, 32, 256, 256]             512\n",
      "           Conv2d-15         [-1, 32, 258, 258]             512\n",
      "           Conv2d-16         [-1, 32, 258, 258]             512\n",
      "    AttentionConv-17         [-1, 32, 256, 256]               0\n",
      "      BatchNorm2d-18         [-1, 32, 256, 256]              64\n",
      "             ReLU-19         [-1, 32, 256, 256]               0\n",
      "           Conv2d-20         [-1, 32, 256, 256]           1,024\n",
      "           Conv2d-21         [-1, 32, 258, 258]           1,024\n",
      "           Conv2d-22         [-1, 32, 258, 258]           1,024\n",
      "    AttentionConv-23         [-1, 32, 256, 256]               0\n",
      "      BatchNorm2d-24         [-1, 32, 256, 256]              64\n",
      "             ReLU-25         [-1, 32, 256, 256]               0\n",
      "        MaxPool2d-26         [-1, 32, 128, 128]               0\n",
      "           Conv2d-27         [-1, 64, 128, 128]           2,048\n",
      "           Conv2d-28         [-1, 64, 130, 130]           2,048\n",
      "           Conv2d-29         [-1, 64, 130, 130]           2,048\n",
      "    AttentionConv-30         [-1, 64, 128, 128]               0\n",
      "      BatchNorm2d-31         [-1, 64, 128, 128]             128\n",
      "             ReLU-32         [-1, 64, 128, 128]               0\n",
      "           Conv2d-33         [-1, 64, 128, 128]           4,096\n",
      "           Conv2d-34         [-1, 64, 130, 130]           4,096\n",
      "           Conv2d-35         [-1, 64, 130, 130]           4,096\n",
      "    AttentionConv-36         [-1, 64, 128, 128]               0\n",
      "      BatchNorm2d-37         [-1, 64, 128, 128]             128\n",
      "             ReLU-38         [-1, 64, 128, 128]               0\n",
      "        MaxPool2d-39           [-1, 64, 64, 64]               0\n",
      "           Conv2d-40           [-1, 64, 64, 64]           4,096\n",
      "           Conv2d-41           [-1, 64, 66, 66]           4,096\n",
      "           Conv2d-42           [-1, 64, 66, 66]           4,096\n",
      "    AttentionConv-43           [-1, 64, 64, 64]               0\n",
      "      BatchNorm2d-44           [-1, 64, 64, 64]             128\n",
      "             ReLU-45           [-1, 64, 64, 64]               0\n",
      "           Conv2d-46           [-1, 64, 64, 64]           4,096\n",
      "           Conv2d-47           [-1, 64, 66, 66]           4,096\n",
      "           Conv2d-48           [-1, 64, 66, 66]           4,096\n",
      "    AttentionConv-49           [-1, 64, 64, 64]               0\n",
      "      BatchNorm2d-50           [-1, 64, 64, 64]             128\n",
      "             ReLU-51           [-1, 64, 64, 64]               0\n",
      "        MaxPool2d-52           [-1, 64, 32, 32]               0\n",
      "           Conv2d-53           [-1, 64, 32, 32]           4,096\n",
      "           Conv2d-54           [-1, 64, 34, 34]           4,096\n",
      "           Conv2d-55           [-1, 64, 34, 34]           4,096\n",
      "    AttentionConv-56           [-1, 64, 32, 32]               0\n",
      "      BatchNorm2d-57           [-1, 64, 32, 32]             128\n",
      "             ReLU-58           [-1, 64, 32, 32]               0\n",
      "           Conv2d-59           [-1, 64, 32, 32]           4,096\n",
      "           Conv2d-60           [-1, 64, 34, 34]           4,096\n",
      "           Conv2d-61           [-1, 64, 34, 34]           4,096\n",
      "    AttentionConv-62           [-1, 64, 32, 32]               0\n",
      "      BatchNorm2d-63           [-1, 64, 32, 32]             128\n",
      "             ReLU-64           [-1, 64, 32, 32]               0\n",
      "        MaxPool2d-65           [-1, 64, 16, 16]               0\n",
      "           Conv2d-66           [-1, 64, 16, 16]           4,096\n",
      "           Conv2d-67           [-1, 64, 18, 18]           4,096\n",
      "           Conv2d-68           [-1, 64, 18, 18]           4,096\n",
      "    AttentionConv-69           [-1, 64, 16, 16]               0\n",
      "      BatchNorm2d-70           [-1, 64, 16, 16]             128\n",
      "             ReLU-71           [-1, 64, 16, 16]               0\n",
      "        MaxPool2d-72             [-1, 64, 8, 8]               0\n",
      "           Linear-73                 [-1, 1024]       4,195,328\n",
      "             ReLU-74                 [-1, 1024]               0\n",
      "           Linear-75                  [-1, 256]         262,400\n",
      "             ReLU-76                  [-1, 256]               0\n",
      "           Linear-77                   [-1, 64]          16,448\n",
      "             ReLU-78                   [-1, 64]               0\n",
      "           Linear-79                   [-1, 15]             975\n",
      "================================================================\n",
      "Total params: 4,561,535\n",
      "Trainable params: 4,561,535\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 1.00\n",
      "Forward/backward pass size (MB): 721.27\n",
      "Params size (MB): 17.40\n",
      "Estimated Total Size (MB): 739.67\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from model import AttNet\n",
    "net1 = AttNet(1,15).cuda()\n",
    "\n",
    "summary(net1, input_size=(1, 512, 512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 256, 256]           3,136\n",
      "       BatchNorm2d-2         [-1, 64, 256, 256]             128\n",
      "              ReLU-3         [-1, 64, 256, 256]               0\n",
      "         MaxPool2d-4         [-1, 64, 128, 128]               0\n",
      "            Conv2d-5         [-1, 64, 128, 128]          36,864\n",
      "       BatchNorm2d-6         [-1, 64, 128, 128]             128\n",
      "              ReLU-7         [-1, 64, 128, 128]               0\n",
      "            Conv2d-8         [-1, 64, 128, 128]          36,864\n",
      "       BatchNorm2d-9         [-1, 64, 128, 128]             128\n",
      "             ReLU-10         [-1, 64, 128, 128]               0\n",
      "       BasicBlock-11         [-1, 64, 128, 128]               0\n",
      "           Conv2d-12         [-1, 64, 128, 128]          36,864\n",
      "      BatchNorm2d-13         [-1, 64, 128, 128]             128\n",
      "             ReLU-14         [-1, 64, 128, 128]               0\n",
      "           Conv2d-15         [-1, 64, 128, 128]          36,864\n",
      "      BatchNorm2d-16         [-1, 64, 128, 128]             128\n",
      "             ReLU-17         [-1, 64, 128, 128]               0\n",
      "       BasicBlock-18         [-1, 64, 128, 128]               0\n",
      "           Conv2d-19          [-1, 128, 64, 64]          73,728\n",
      "      BatchNorm2d-20          [-1, 128, 64, 64]             256\n",
      "             ReLU-21          [-1, 128, 64, 64]               0\n",
      "           Conv2d-22          [-1, 128, 64, 64]         147,456\n",
      "      BatchNorm2d-23          [-1, 128, 64, 64]             256\n",
      "           Conv2d-24          [-1, 128, 64, 64]           8,192\n",
      "      BatchNorm2d-25          [-1, 128, 64, 64]             256\n",
      "             ReLU-26          [-1, 128, 64, 64]               0\n",
      "       BasicBlock-27          [-1, 128, 64, 64]               0\n",
      "           Conv2d-28          [-1, 128, 64, 64]         147,456\n",
      "      BatchNorm2d-29          [-1, 128, 64, 64]             256\n",
      "             ReLU-30          [-1, 128, 64, 64]               0\n",
      "           Conv2d-31          [-1, 128, 64, 64]         147,456\n",
      "      BatchNorm2d-32          [-1, 128, 64, 64]             256\n",
      "             ReLU-33          [-1, 128, 64, 64]               0\n",
      "       BasicBlock-34          [-1, 128, 64, 64]               0\n",
      "           Conv2d-35          [-1, 256, 32, 32]         294,912\n",
      "      BatchNorm2d-36          [-1, 256, 32, 32]             512\n",
      "             ReLU-37          [-1, 256, 32, 32]               0\n",
      "           Conv2d-38          [-1, 256, 32, 32]         589,824\n",
      "      BatchNorm2d-39          [-1, 256, 32, 32]             512\n",
      "           Conv2d-40          [-1, 256, 32, 32]          32,768\n",
      "      BatchNorm2d-41          [-1, 256, 32, 32]             512\n",
      "             ReLU-42          [-1, 256, 32, 32]               0\n",
      "       BasicBlock-43          [-1, 256, 32, 32]               0\n",
      "           Conv2d-44          [-1, 256, 32, 32]         589,824\n",
      "      BatchNorm2d-45          [-1, 256, 32, 32]             512\n",
      "             ReLU-46          [-1, 256, 32, 32]               0\n",
      "           Conv2d-47          [-1, 256, 32, 32]         589,824\n",
      "      BatchNorm2d-48          [-1, 256, 32, 32]             512\n",
      "             ReLU-49          [-1, 256, 32, 32]               0\n",
      "       BasicBlock-50          [-1, 256, 32, 32]               0\n",
      "           Conv2d-51          [-1, 512, 16, 16]       1,179,648\n",
      "      BatchNorm2d-52          [-1, 512, 16, 16]           1,024\n",
      "             ReLU-53          [-1, 512, 16, 16]               0\n",
      "           Conv2d-54          [-1, 512, 16, 16]       2,359,296\n",
      "      BatchNorm2d-55          [-1, 512, 16, 16]           1,024\n",
      "           Conv2d-56          [-1, 512, 16, 16]         131,072\n",
      "      BatchNorm2d-57          [-1, 512, 16, 16]           1,024\n",
      "             ReLU-58          [-1, 512, 16, 16]               0\n",
      "       BasicBlock-59          [-1, 512, 16, 16]               0\n",
      "           Conv2d-60          [-1, 512, 16, 16]       2,359,296\n",
      "      BatchNorm2d-61          [-1, 512, 16, 16]           1,024\n",
      "             ReLU-62          [-1, 512, 16, 16]               0\n",
      "           Conv2d-63          [-1, 512, 16, 16]       2,359,296\n",
      "      BatchNorm2d-64          [-1, 512, 16, 16]           1,024\n",
      "             ReLU-65          [-1, 512, 16, 16]               0\n",
      "       BasicBlock-66          [-1, 512, 16, 16]               0\n",
      "AdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n",
      "           Linear-68                   [-1, 15]           7,695\n",
      "================================================================\n",
      "Total params: 11,177,935\n",
      "Trainable params: 11,177,935\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 1.00\n",
      "Forward/backward pass size (MB): 328.00\n",
      "Params size (MB): 42.64\n",
      "Estimated Total Size (MB): 371.64\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "net2 = models.resnet18(pretrained=False)\n",
    "net2.conv1 = torch.nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "num_ftrs = net2.fc.in_features\n",
    "net2.fc = nn.Linear(num_ftrs, 15)\n",
    "net = net2.cuda()\n",
    "summary(net, input_size=(1, 512, 512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 256, 256]           3,136\n",
      "       BatchNorm2d-2         [-1, 64, 256, 256]             128\n",
      "              ReLU-3         [-1, 64, 256, 256]               0\n",
      "         MaxPool2d-4         [-1, 64, 128, 128]               0\n",
      "            Conv2d-5         [-1, 64, 128, 128]           4,096\n",
      "       BatchNorm2d-6         [-1, 64, 128, 128]             128\n",
      "              ReLU-7         [-1, 64, 128, 128]               0\n",
      "            Conv2d-8         [-1, 64, 128, 128]          36,864\n",
      "       BatchNorm2d-9         [-1, 64, 128, 128]             128\n",
      "             ReLU-10         [-1, 64, 128, 128]               0\n",
      "           Conv2d-11        [-1, 256, 128, 128]          16,384\n",
      "      BatchNorm2d-12        [-1, 256, 128, 128]             512\n",
      "           Conv2d-13        [-1, 256, 128, 128]          16,384\n",
      "      BatchNorm2d-14        [-1, 256, 128, 128]             512\n",
      "             ReLU-15        [-1, 256, 128, 128]               0\n",
      "       Bottleneck-16        [-1, 256, 128, 128]               0\n",
      "           Conv2d-17         [-1, 64, 128, 128]          16,384\n",
      "      BatchNorm2d-18         [-1, 64, 128, 128]             128\n",
      "             ReLU-19         [-1, 64, 128, 128]               0\n",
      "           Conv2d-20         [-1, 64, 128, 128]          36,864\n",
      "      BatchNorm2d-21         [-1, 64, 128, 128]             128\n",
      "             ReLU-22         [-1, 64, 128, 128]               0\n",
      "           Conv2d-23        [-1, 256, 128, 128]          16,384\n",
      "      BatchNorm2d-24        [-1, 256, 128, 128]             512\n",
      "             ReLU-25        [-1, 256, 128, 128]               0\n",
      "       Bottleneck-26        [-1, 256, 128, 128]               0\n",
      "           Conv2d-27         [-1, 64, 128, 128]          16,384\n",
      "      BatchNorm2d-28         [-1, 64, 128, 128]             128\n",
      "             ReLU-29         [-1, 64, 128, 128]               0\n",
      "           Conv2d-30         [-1, 64, 128, 128]          36,864\n",
      "      BatchNorm2d-31         [-1, 64, 128, 128]             128\n",
      "             ReLU-32         [-1, 64, 128, 128]               0\n",
      "           Conv2d-33        [-1, 256, 128, 128]          16,384\n",
      "      BatchNorm2d-34        [-1, 256, 128, 128]             512\n",
      "             ReLU-35        [-1, 256, 128, 128]               0\n",
      "       Bottleneck-36        [-1, 256, 128, 128]               0\n",
      "           Conv2d-37        [-1, 128, 128, 128]          32,768\n",
      "      BatchNorm2d-38        [-1, 128, 128, 128]             256\n",
      "             ReLU-39        [-1, 128, 128, 128]               0\n",
      "           Conv2d-40          [-1, 128, 64, 64]         147,456\n",
      "      BatchNorm2d-41          [-1, 128, 64, 64]             256\n",
      "             ReLU-42          [-1, 128, 64, 64]               0\n",
      "           Conv2d-43          [-1, 512, 64, 64]          65,536\n",
      "      BatchNorm2d-44          [-1, 512, 64, 64]           1,024\n",
      "           Conv2d-45          [-1, 512, 64, 64]         131,072\n",
      "      BatchNorm2d-46          [-1, 512, 64, 64]           1,024\n",
      "             ReLU-47          [-1, 512, 64, 64]               0\n",
      "       Bottleneck-48          [-1, 512, 64, 64]               0\n",
      "           Conv2d-49          [-1, 128, 64, 64]          65,536\n",
      "      BatchNorm2d-50          [-1, 128, 64, 64]             256\n",
      "             ReLU-51          [-1, 128, 64, 64]               0\n",
      "           Conv2d-52          [-1, 128, 64, 64]         147,456\n",
      "      BatchNorm2d-53          [-1, 128, 64, 64]             256\n",
      "             ReLU-54          [-1, 128, 64, 64]               0\n",
      "           Conv2d-55          [-1, 512, 64, 64]          65,536\n",
      "      BatchNorm2d-56          [-1, 512, 64, 64]           1,024\n",
      "             ReLU-57          [-1, 512, 64, 64]               0\n",
      "       Bottleneck-58          [-1, 512, 64, 64]               0\n",
      "           Conv2d-59          [-1, 128, 64, 64]          65,536\n",
      "      BatchNorm2d-60          [-1, 128, 64, 64]             256\n",
      "             ReLU-61          [-1, 128, 64, 64]               0\n",
      "           Conv2d-62          [-1, 128, 64, 64]         147,456\n",
      "      BatchNorm2d-63          [-1, 128, 64, 64]             256\n",
      "             ReLU-64          [-1, 128, 64, 64]               0\n",
      "           Conv2d-65          [-1, 512, 64, 64]          65,536\n",
      "      BatchNorm2d-66          [-1, 512, 64, 64]           1,024\n",
      "             ReLU-67          [-1, 512, 64, 64]               0\n",
      "       Bottleneck-68          [-1, 512, 64, 64]               0\n",
      "           Conv2d-69          [-1, 128, 64, 64]          65,536\n",
      "      BatchNorm2d-70          [-1, 128, 64, 64]             256\n",
      "             ReLU-71          [-1, 128, 64, 64]               0\n",
      "           Conv2d-72          [-1, 128, 64, 64]         147,456\n",
      "      BatchNorm2d-73          [-1, 128, 64, 64]             256\n",
      "             ReLU-74          [-1, 128, 64, 64]               0\n",
      "           Conv2d-75          [-1, 512, 64, 64]          65,536\n",
      "      BatchNorm2d-76          [-1, 512, 64, 64]           1,024\n",
      "             ReLU-77          [-1, 512, 64, 64]               0\n",
      "       Bottleneck-78          [-1, 512, 64, 64]               0\n",
      "           Conv2d-79          [-1, 256, 64, 64]         131,072\n",
      "      BatchNorm2d-80          [-1, 256, 64, 64]             512\n",
      "             ReLU-81          [-1, 256, 64, 64]               0\n",
      "           Conv2d-82          [-1, 256, 32, 32]         589,824\n",
      "      BatchNorm2d-83          [-1, 256, 32, 32]             512\n",
      "             ReLU-84          [-1, 256, 32, 32]               0\n",
      "           Conv2d-85         [-1, 1024, 32, 32]         262,144\n",
      "      BatchNorm2d-86         [-1, 1024, 32, 32]           2,048\n",
      "           Conv2d-87         [-1, 1024, 32, 32]         524,288\n",
      "      BatchNorm2d-88         [-1, 1024, 32, 32]           2,048\n",
      "             ReLU-89         [-1, 1024, 32, 32]               0\n",
      "       Bottleneck-90         [-1, 1024, 32, 32]               0\n",
      "           Conv2d-91          [-1, 256, 32, 32]         262,144\n",
      "      BatchNorm2d-92          [-1, 256, 32, 32]             512\n",
      "             ReLU-93          [-1, 256, 32, 32]               0\n",
      "           Conv2d-94          [-1, 256, 32, 32]         589,824\n",
      "      BatchNorm2d-95          [-1, 256, 32, 32]             512\n",
      "             ReLU-96          [-1, 256, 32, 32]               0\n",
      "           Conv2d-97         [-1, 1024, 32, 32]         262,144\n",
      "      BatchNorm2d-98         [-1, 1024, 32, 32]           2,048\n",
      "             ReLU-99         [-1, 1024, 32, 32]               0\n",
      "      Bottleneck-100         [-1, 1024, 32, 32]               0\n",
      "          Conv2d-101          [-1, 256, 32, 32]         262,144\n",
      "     BatchNorm2d-102          [-1, 256, 32, 32]             512\n",
      "            ReLU-103          [-1, 256, 32, 32]               0\n",
      "          Conv2d-104          [-1, 256, 32, 32]         589,824\n",
      "     BatchNorm2d-105          [-1, 256, 32, 32]             512\n",
      "            ReLU-106          [-1, 256, 32, 32]               0\n",
      "          Conv2d-107         [-1, 1024, 32, 32]         262,144\n",
      "     BatchNorm2d-108         [-1, 1024, 32, 32]           2,048\n",
      "            ReLU-109         [-1, 1024, 32, 32]               0\n",
      "      Bottleneck-110         [-1, 1024, 32, 32]               0\n",
      "          Conv2d-111          [-1, 256, 32, 32]         262,144\n",
      "     BatchNorm2d-112          [-1, 256, 32, 32]             512\n",
      "            ReLU-113          [-1, 256, 32, 32]               0\n",
      "          Conv2d-114          [-1, 256, 32, 32]         589,824\n",
      "     BatchNorm2d-115          [-1, 256, 32, 32]             512\n",
      "            ReLU-116          [-1, 256, 32, 32]               0\n",
      "          Conv2d-117         [-1, 1024, 32, 32]         262,144\n",
      "     BatchNorm2d-118         [-1, 1024, 32, 32]           2,048\n",
      "            ReLU-119         [-1, 1024, 32, 32]               0\n",
      "      Bottleneck-120         [-1, 1024, 32, 32]               0\n",
      "          Conv2d-121          [-1, 256, 32, 32]         262,144\n",
      "     BatchNorm2d-122          [-1, 256, 32, 32]             512\n",
      "            ReLU-123          [-1, 256, 32, 32]               0\n",
      "          Conv2d-124          [-1, 256, 32, 32]         589,824\n",
      "     BatchNorm2d-125          [-1, 256, 32, 32]             512\n",
      "            ReLU-126          [-1, 256, 32, 32]               0\n",
      "          Conv2d-127         [-1, 1024, 32, 32]         262,144\n",
      "     BatchNorm2d-128         [-1, 1024, 32, 32]           2,048\n",
      "            ReLU-129         [-1, 1024, 32, 32]               0\n",
      "      Bottleneck-130         [-1, 1024, 32, 32]               0\n",
      "          Conv2d-131          [-1, 256, 32, 32]         262,144\n",
      "     BatchNorm2d-132          [-1, 256, 32, 32]             512\n",
      "            ReLU-133          [-1, 256, 32, 32]               0\n",
      "          Conv2d-134          [-1, 256, 32, 32]         589,824\n",
      "     BatchNorm2d-135          [-1, 256, 32, 32]             512\n",
      "            ReLU-136          [-1, 256, 32, 32]               0\n",
      "          Conv2d-137         [-1, 1024, 32, 32]         262,144\n",
      "     BatchNorm2d-138         [-1, 1024, 32, 32]           2,048\n",
      "            ReLU-139         [-1, 1024, 32, 32]               0\n",
      "      Bottleneck-140         [-1, 1024, 32, 32]               0\n",
      "          Conv2d-141          [-1, 512, 32, 32]         524,288\n",
      "     BatchNorm2d-142          [-1, 512, 32, 32]           1,024\n",
      "            ReLU-143          [-1, 512, 32, 32]               0\n",
      "          Conv2d-144          [-1, 512, 16, 16]       2,359,296\n",
      "     BatchNorm2d-145          [-1, 512, 16, 16]           1,024\n",
      "            ReLU-146          [-1, 512, 16, 16]               0\n",
      "          Conv2d-147         [-1, 2048, 16, 16]       1,048,576\n",
      "     BatchNorm2d-148         [-1, 2048, 16, 16]           4,096\n",
      "          Conv2d-149         [-1, 2048, 16, 16]       2,097,152\n",
      "     BatchNorm2d-150         [-1, 2048, 16, 16]           4,096\n",
      "            ReLU-151         [-1, 2048, 16, 16]               0\n",
      "      Bottleneck-152         [-1, 2048, 16, 16]               0\n",
      "          Conv2d-153          [-1, 512, 16, 16]       1,048,576\n",
      "     BatchNorm2d-154          [-1, 512, 16, 16]           1,024\n",
      "            ReLU-155          [-1, 512, 16, 16]               0\n",
      "          Conv2d-156          [-1, 512, 16, 16]       2,359,296\n",
      "     BatchNorm2d-157          [-1, 512, 16, 16]           1,024\n",
      "            ReLU-158          [-1, 512, 16, 16]               0\n",
      "          Conv2d-159         [-1, 2048, 16, 16]       1,048,576\n",
      "     BatchNorm2d-160         [-1, 2048, 16, 16]           4,096\n",
      "            ReLU-161         [-1, 2048, 16, 16]               0\n",
      "      Bottleneck-162         [-1, 2048, 16, 16]               0\n",
      "          Conv2d-163          [-1, 512, 16, 16]       1,048,576\n",
      "     BatchNorm2d-164          [-1, 512, 16, 16]           1,024\n",
      "            ReLU-165          [-1, 512, 16, 16]               0\n",
      "          Conv2d-166          [-1, 512, 16, 16]       2,359,296\n",
      "     BatchNorm2d-167          [-1, 512, 16, 16]           1,024\n",
      "            ReLU-168          [-1, 512, 16, 16]               0\n",
      "          Conv2d-169         [-1, 2048, 16, 16]       1,048,576\n",
      "     BatchNorm2d-170         [-1, 2048, 16, 16]           4,096\n",
      "            ReLU-171         [-1, 2048, 16, 16]               0\n",
      "      Bottleneck-172         [-1, 2048, 16, 16]               0\n",
      "AdaptiveAvgPool2d-173           [-1, 2048, 1, 1]               0\n",
      "          Linear-174                   [-1, 15]          30,735\n",
      "================================================================\n",
      "Total params: 23,532,495\n",
      "Trainable params: 23,532,495\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 1.00\n",
      "Forward/backward pass size (MB): 1497.02\n",
      "Params size (MB): 89.77\n",
      "Estimated Total Size (MB): 1587.79\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "net = models.resnet50(pretrained=False)\n",
    "net.conv1 = torch.nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "num_ftrs = net.fc.in_features\n",
    "net.fc = nn.Linear(num_ftrs, 15)\n",
    "net = net.cuda()\n",
    "summary(net, input_size=(1, 512, 512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.30 MB\n"
     ]
    }
   ],
   "source": [
    "print(\"%.2f MB\" %(os.path.getsize('/home/rakshith/miccai_2022/model_weights/classification/ssa_adam_1e4_e15_bce/model_best.pt')/1e6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.profiler import profile, record_function, ProfilerActivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.randn(1,1,512,512).cuda()\n",
    "net = net.cuda()\n",
    "with profile(activities=[ProfilerActivity.CPU], record_shapes=True) as prof:\n",
    "    with record_function(\"model_inference\"):\n",
    "        net(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                        model_inference        20.36%       8.093ms        99.92%      39.717ms      39.717ms             1  \n",
      "                                           aten::conv2d         5.29%       2.103ms        37.26%      14.809ms     279.415us            53  \n",
      "                                      aten::convolution         1.64%     653.000us        31.96%      12.706ms     239.736us            53  \n",
      "                                     aten::_convolution         2.16%     858.000us        30.32%      12.053ms     227.415us            53  \n",
      "                                       aten::batch_norm         1.54%     614.000us        28.63%      11.379ms     214.698us            53  \n",
      "                                aten::cudnn_convolution        12.10%       4.811ms        28.16%      11.195ms     211.226us            53  \n",
      "                           aten::_batch_norm_impl_index         1.56%     621.000us        27.08%      10.765ms     203.113us            53  \n",
      "                                 aten::cudnn_batch_norm        11.95%       4.749ms        25.52%      10.144ms     191.396us            53  \n",
      "                                            aten::empty        15.40%       6.120ms        20.75%       8.248ms      25.457us           324  \n",
      "                                       aten::empty_like         1.15%     458.000us         7.90%       3.139ms      59.226us            53  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 39.750ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                  model_inference        11.04%     149.548ms       100.00%        1.354s        1.354s             1  \n",
      "                     aten::conv2d         0.03%     474.000us        23.45%     317.598ms       9.624ms            33  \n",
      "                aten::convolution         0.03%     410.000us        23.42%     317.124ms       9.610ms            33  \n",
      "               aten::_convolution         1.74%      23.584ms        23.38%     316.714ms       9.597ms            33  \n",
      "                        aten::cat         0.02%     318.000us        22.78%     308.456ms      28.041ms            11  \n",
      "                       aten::_cat        22.73%     307.806ms        22.75%     308.138ms      28.013ms            11  \n",
      "       aten::_convolution_nogroup         0.75%      10.217ms        21.64%     293.130ms       8.883ms            33  \n",
      "                      aten::copy_        21.31%     288.649ms        21.31%     288.649ms       7.801ms            37  \n",
      "                 aten::contiguous         0.02%     239.000us        21.27%     288.116ms      13.096ms            22  \n",
      "                aten::thnn_conv2d         1.07%      14.467ms        19.05%     257.988ms       7.818ms            33  \n",
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.354s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# AttentionNet \n",
    "inputs = torch.randn(1,1,512,512)\n",
    "net1 = net1.cpu()\n",
    "with profile(activities=[ProfilerActivity.CPU], record_shapes=True) as prof:\n",
    "    with record_function(\"model_inference\"):\n",
    "        net(inputs)\n",
    "print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                  model_inference         0.85%       4.461ms        99.99%     527.354ms     527.354ms             1  \n",
      "                     aten::conv2d         0.04%     217.000us        83.83%     442.119ms      22.106ms            20  \n",
      "                aten::convolution         0.05%     265.000us        83.79%     441.902ms      22.095ms            20  \n",
      "               aten::_convolution         0.08%     423.000us        83.74%     441.637ms      22.082ms            20  \n",
      "         aten::mkldnn_convolution        83.62%     441.002ms        83.66%     441.214ms      22.061ms            20  \n",
      "                 aten::max_pool2d         0.00%      17.000us         9.48%      50.011ms      50.011ms             1  \n",
      "    aten::max_pool2d_with_indices         9.48%      49.974ms         9.48%      49.994ms      49.994ms             1  \n",
      "                 aten::batch_norm         0.03%     173.000us         5.25%      27.663ms       1.383ms            20  \n",
      "     aten::_batch_norm_impl_index         0.06%     314.000us         5.21%      27.490ms       1.375ms            20  \n",
      "          aten::native_batch_norm         3.57%      18.842ms         5.14%      27.134ms       1.357ms            20  \n",
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 527.392ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.randn(1,1,512,512)\n",
    "net2 = net2.cpu()\n",
    "with profile(activities=[ProfilerActivity.CPU], record_shapes=True) as prof:\n",
    "    with record_function(\"model_inference\"):\n",
    "        net2(inputs)\n",
    "print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "02d518ac1fc09ef5dc4eabe4d9fe87a26d09bc9551d9c7d27686a6ac8a51ace1"
  },
  "kernelspec": {
   "display_name": "Python 3.7.6 ('rak-env': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
