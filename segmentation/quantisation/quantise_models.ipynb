{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import torch.nn.functional as F\n",
    "# from self_attn_sumnet import FullAttenSUMNet, SUMNet\n",
    "# from unet import UNet\n",
    "from sasa_einsum_sumnet import FullAttenSUMNet\n",
    "from train import test\n",
    "from tqdm import tqdm\n",
    "import torchmetrics.functional as TF\n",
    "from dataloader import LiverDataLoader\n",
    "from torch.utils import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.manual_seed(0)\n",
    "# torch.cuda.manual_seed(0)\n",
    "# torch.cuda.manual_seed_all(0)\n",
    "# torch.backends.cudnn.deterministic = True\n",
    "# torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_demo = FullAttenSUMNet(in_ch=1, out_ch=2)\n",
    "# print(model_demo.state_dict()['conv1.rel_h'][10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FullAttenSUMNet(in_ch=1, out_ch=2)\n",
    "model_path = '/home/rakshith/miccai_2022/model_weights/segmentation/ssa_seg_adam_1e4_e15_re/'\n",
    "checkpoint= torch.load(model_path+'model_best_dice.pt')\n",
    "# checkpoint= torch.load(model_path+'model_last.pt')\n",
    "model.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = 'ssa_seg_adam_1e4_e15_re'\n",
    "# config = {\n",
    "#         \"savepath\":\"/home/rakshith/miccai_2022/model_weights/segmentation/\"+model_name+\"/\",\n",
    "#         \"datapath\": \"/home/rakshith/Datasets/Task03_Liver/np_array_slices/\",\n",
    "#         \"data_split\": \"/home/rakshith/miccai_2022/segmentation/train_valid_split.json\",\n",
    "#         \"out_ch\" : 2,\n",
    "#         \"model_name\":model_name,\n",
    "#         \"model\" : 'ssa',\n",
    "#         \"quantised\": 1\n",
    "#     }\n",
    "    \n",
    "# data_path = config['datapath']\n",
    "# load_path = config['savepath']\n",
    "# data_split = config['data_split']\n",
    "# with open(data_split) as f3:\n",
    "#         data_split = json.load(f3)\n",
    "# file_list = data_split['valid'][:5]\n",
    "\n",
    "# # train_split = data_split['train'][:5]\n",
    "# # train_d_set = LiverDataLoader(datapath=data_path, file_list=train_split, is_transform=True, augmentation=None)\n",
    "\n",
    "# # train_data_loader = data.DataLoader(\n",
    "#                                 # train_d_set, batch_size=1,\n",
    "#                                 # shuffle=True, num_workers=8, pin_memory=True,drop_last=True)\n",
    "\n",
    "# testDset = LiverDataLoader(datapath=data_path, file_list=file_list,is_transform=True)\n",
    "\n",
    "# testDataLoader = data.DataLoader(\n",
    "#                             testDset, batch_size=1, drop_last=True,\n",
    "#                             shuffle=False, num_workers=2, pin_memory=True)\n",
    "# model.cuda().eval()\n",
    "\n",
    "# for datasample in testDataLoader:\n",
    "#     inputs, mask, fname = datasample\n",
    "#     print(fname)\n",
    "    \n",
    "#     inputs = inputs.cuda()\n",
    "#     mask = mask.cuda()\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         seg_out = model(inputs)\n",
    "#         net_out_sf = F.softmax(seg_out.data, dim=1)\n",
    "\n",
    "#         preds = torch.argmax(net_out_sf, dim=1)\n",
    "#         test_dice = TF.f1_score(preds.flatten().long(), mask[:,1].flatten().long(), num_classes=2, ignore_index=0)\n",
    "#         print(test_dice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# plt.subplot(121)\n",
    "# plt.imshow(mask[:,1].squeeze(0).cpu(),cmap='gray')\n",
    "# plt.subplot(122)\n",
    "# plt.imshow(preds.squeeze(0).detach().cpu(),cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = 'ssa_seg_adam_1e4_e15_re'\n",
    "# config = {\n",
    "#         \"savepath\":\"/home/rakshith/miccai_2022/model_weights/segmentation/\"+model_name+\"/\",\n",
    "#         \"datapath\": \"/home/rakshith/Datasets/Task03_Liver/np_array_slices/\",\n",
    "#         \"data_split\": \"/home/rakshith/miccai_2022/segmentation/train_valid_split.json\",\n",
    "#         \"out_ch\" : 2,\n",
    "#         \"model_name\":model_name,\n",
    "#         \"model\" : 'ssa',\n",
    "#         \"quantised\": 1\n",
    "#     }\n",
    "# model.eval()\n",
    "# test(config, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.quantization import get_default_qconfig\n",
    "# Note that this is temporary, we'll expose these functions to torch.quantization after official releasee\n",
    "from torch.quantization.quantize_fx import prepare_fx, convert_fx\n",
    "import json\n",
    "from torch.utils import data\n",
    "from dataloader import LiverDataLoader\n",
    "\n",
    "model_name = 'ssa_seg_adam_1e4_e15_re'\n",
    "config = {\n",
    "        \"savepath\":\"/home/rakshith/miccai_2022/model_weights/segmentation/\"+model_name+\"/\",\n",
    "        \"datapath\": \"/home/rakshith/Datasets/Task03_Liver/np_array_slices/\",\n",
    "        \"data_split\": \"/home/rakshith/miccai_2022/segmentation/train_valid_split.json\",\n",
    "        \"out_ch\" : 2,\n",
    "        \"model_name\":model_name,\n",
    "        \"model\" : 'ssa_quant',\n",
    "        \"quantised\": 1\n",
    "    }\n",
    "\n",
    "data_split = config['data_split']\n",
    "\n",
    "with open('/home/rakshith/miccai_2022/segmentation/train_valid_split.json') as f3:\n",
    "    data_split = json.load(f3)\n",
    "file_list = data_split['valid'][:60]\n",
    "data_path = '/home/rakshith/Datasets/Task03_Liver/np_array_slices/'\n",
    "\n",
    "testDset = LiverDataLoader(datapath=data_path, file_list=file_list,is_transform=True)\n",
    "\n",
    "testDataLoader = data.DataLoader(\n",
    "                            testDset, batch_size=1, drop_last=True,\n",
    "                            shuffle=False, num_workers=2, pin_memory=True)\n",
    "model.eval()\n",
    "qconfig = get_default_qconfig(\"fbgemm\")\n",
    "qconfig_dict = {\"\": qconfig}\n",
    "prepare_custom_config_dict = {\n",
    "    # option 1\n",
    "    \"non_traceable_module_name\": [\"pool\"],\n",
    "}\n",
    "def calibrate(model, data_loader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for image, target,_ in tqdm(data_loader):\n",
    "            model(image)\n",
    "\n",
    "prepared_model = prepare_fx(model.cpu(), qconfig_dict,prepare_custom_config_dict=prepare_custom_config_dict,)  # fuse modules and insert observers\n",
    "calibrate(prepared_model, testDataLoader)  # run calibration on sample data\n",
    "quantized_model = convert_fx(prepared_model)  # convert the calibrated model to a quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for datasample in testDataLoader:\n",
    "    inputs, mask, fname = datasample\n",
    "    print(fname)\n",
    "    \n",
    "    # inputs = inputs\n",
    "    # mask = mask\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        seg_out = quantized_model(inputs)\n",
    "        net_out_sf = F.softmax(seg_out.data, dim=1)\n",
    "\n",
    "        preds = torch.argmax(net_out_sf, dim=1)\n",
    "        test_dice = TF.f1_score(preds.flatten().long(), mask[:,1].flatten().long(), num_classes=2, ignore_index=0)\n",
    "        print(test_dice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backend = \"fbgemm\"\n",
    "model.eval()\n",
    "model.qconfig = torch.quantization.get_default_qconfig(backend)\n",
    "torch.backends.quantized.engine = backend\n",
    "model_static_quantized = torch.quantization.prepare(model, inplace=False)\n",
    "model_static_quantized = torch.quantization.convert(model_static_quantized, inplace=False)\n",
    "state = {'state_dict': model_static_quantized.state_dict() }\n",
    "save_path = '/home/rakshith/miccai_2022/model_weights/segmentation/ssa_seg_adam_1e4_e15_re_quantised/'\n",
    "if not os.path.exists(save_path):\n",
    "    os.mkdir(save_path)\n",
    "torch.save(state, save_path+'model_best_dice.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'ssa_seg_adam_1e4_e15_re_quantised'\n",
    "config = {\n",
    "        \"savepath\":\"/home/rakshith/miccai_2022/model_weights/segmentation/\"+model_name+\"/\",\n",
    "        \"datapath\": \"/home/rakshith/Datasets/Task03_Liver/np_array_slices/\",\n",
    "        \"data_split\": \"/home/rakshith/miccai_2022/segmentation/train_valid_split.json\",\n",
    "        \"out_ch\" : 2,\n",
    "        \"model_name\":model_name,\n",
    "        \"model\" : 'ssa_quant',\n",
    "        \"quantised\": 1\n",
    "    }\n",
    "test(config, model_static_quantized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# torch.cuda.set_device(1)\n",
    "model = SUMNet(in_ch=1,out_ch=2)\n",
    "model_path = '/home/rakshith/miccai_2022/model_weights/segmentation/sumnet_seg_adam_1e4_e15/'\n",
    "model.load_state_dict(torch.load(model_path+'model_best_seg.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model= model.cuda()\n",
    "model_name = 'sumnet_seg_adam_1e4_e15_re_quantised'\n",
    "config = {\n",
    "        \"savepath\":\"/home/rakshith/miccai_2022/model_weights/segmentation/\"+model_name+\"/\",\n",
    "        \"datapath\": \"/home/rakshith/Datasets/Task03_Liver/np_array_slices/\",\n",
    "        \"data_split\": \"/home/rakshith/miccai_2022/segmentation/train_valid_split.json\",\n",
    "        \"out_ch\" : 2,\n",
    "        \"model_name\":model_name,\n",
    "        \"model\" : 'sumnet_quant',\n",
    "        \"quantised\": 1\n",
    "    }\n",
    "test(config, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.quantization import get_default_qconfig\n",
    "# Note that this is temporary, we'll expose these functions to torch.quantization after official releasee\n",
    "from torch.quantization.quantize_fx import prepare_fx, convert_fx\n",
    "import json\n",
    "from torch.utils import data\n",
    "from dataloader import LiverDataLoader\n",
    "\n",
    "data_split = config['data_split']\n",
    "\n",
    "with open('/home/rakshith/miccai_2022/segmentation/train_valid_split.json') as f3:\n",
    "    data_split = json.load(f3)\n",
    "file_list = data_split['valid'][:10]\n",
    "data_path = '/home/rakshith/Datasets/Task03_Liver/np_array_slices/'\n",
    "\n",
    "testDset = LiverDataLoader(datapath=data_path, file_list=file_list,is_transform=True)\n",
    "\n",
    "testDataLoader = data.DataLoader(\n",
    "                            testDset, batch_size=1, drop_last=True,\n",
    "                            shuffle=False, num_workers=2, pin_memory=True)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "qconfig = get_default_qconfig(\"fbgemm\")\n",
    "qconfig_dict = {\"\": qconfig}\n",
    "prepare_custom_config_dict = {\n",
    "    # option 1\n",
    "    \"non_traceable_module_name\": [\"pool\"],\n",
    "}\n",
    "def calibrate(model, data_loader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for image, target,_ in data_loader:\n",
    "            model(image)\n",
    "\n",
    "prepared_model = prepare_fx(model, qconfig_dict,prepare_custom_config_dict=prepare_custom_config_dict,)  # fuse modules and insert observers\n",
    "calibrate(prepared_model, testDataLoader)  # run calibration on sample data\n",
    "quantized_model = convert_fx(prepared_model)  # convert the calibrated model to a quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(quantized_model.state_dict(),'sumnet_quantized.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(config, quantized_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backend = \"fbgemm\"\n",
    "model.eval()\n",
    "model.qconfig = torch.quantization.get_default_qconfig(backend)\n",
    "torch.backends.quantized.engine = backend\n",
    "model_static_quantized_sumnet = torch.quantization.prepare(model, inplace=True)\n",
    "model_static_quantized_sumnet = torch.quantization.convert(model_static_quantized_sumnet, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet(n_channels=1,n_classes=2)\n",
    "model_path = '/home/rakshith/miccai_2022/model_weights/segmentation/unet_seg_adam_1e4_e15/'\n",
    "model.load_state_dict(torch.load(model_path+'model_best_seg.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backend = \"qnnpack\"\n",
    "model.eval()\n",
    "model.qconfig = torch.quantization.get_default_qconfig(backend)\n",
    "torch.backends.quantized.engine = backend\n",
    "model_static_quantized_unet = torch.quantization.prepare(model, inplace=True)\n",
    "model_static_quantized_unet = torch.quantization.convert(model_static_quantized_unet, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.quantization import get_default_qconfig\n",
    "# Note that this is temporary, we'll expose these functions to torch.quantization after official releasee\n",
    "from torch.quantization.quantize_fx import prepare_fx, convert_fx\n",
    "import json\n",
    "from torch.utils import data\n",
    "from dataloader import LiverDataLoader\n",
    "\n",
    "model_name = 'unet_seg_adam_1e4_e15_re_quantised'\n",
    "config = {\n",
    "        \"savepath\":\"/home/rakshith/miccai_2022/model_weights/segmentation/\"+model_name+\"/\",\n",
    "        \"datapath\": \"/home/rakshith/Datasets/Task03_Liver/np_array_slices/\",\n",
    "        \"data_split\": \"/home/rakshith/miccai_2022/segmentation/train_valid_split.json\",\n",
    "        \"out_ch\" : 2,\n",
    "        \"model_name\":model_name,\n",
    "        \"model\" : 'unet_quant',\n",
    "        \"quantised\": 1\n",
    "    }\n",
    "\n",
    "data_split = config['data_split']\n",
    "\n",
    "with open('/home/rakshith/miccai_2022/segmentation/train_valid_split.json') as f3:\n",
    "    data_split = json.load(f3)\n",
    "file_list = data_split['valid'][:10]\n",
    "data_path = '/home/rakshith/Datasets/Task03_Liver/np_array_slices/'\n",
    "\n",
    "testDset = LiverDataLoader(datapath=data_path, file_list=file_list,is_transform=True)\n",
    "\n",
    "testDataLoader = data.DataLoader(\n",
    "                            testDset, batch_size=1, drop_last=True,\n",
    "                            shuffle=False, num_workers=2, pin_memory=True)\n",
    "model.eval()\n",
    "qconfig = get_default_qconfig(\"qnnpack\")\n",
    "qconfig_dict = {\"\": qconfig}\n",
    "prepare_custom_config_dict = {\n",
    "    # option 1\n",
    "    \"non_traceable_module_name\": [\"up1\",\"up2\",\"up3\",\"up4\",\"ConvTranspose2d\"],\n",
    "}\n",
    "def calibrate(model, data_loader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for image, target,_ in data_loader:\n",
    "            model(image)\n",
    "\n",
    "prepared_model = prepare_fx(model, qconfig_dict,prepare_custom_config_dict=prepare_custom_config_dict,)  # fuse modules and insert observers\n",
    "calibrate(prepared_model, testDataLoader)  # run calibration on sample data\n",
    "quantized_model_unet = convert_fx(prepared_model)  # convert the calibrated model to a quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test(config, quantized_model_unet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_model_size(mdl):\n",
    "    torch.save(mdl.state_dict(), config['savepath']+\"resnet50_quant.pth\")\n",
    "    print(\"%.2f MB\" %(os.path.getsize(config['savepath']+\"resnet50_quant.pth\")/1e6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_model_size(model_static_quantized)\n",
    "print_model_size(model_static_quantized_unet)\n",
    "print_model_size(model_static_quantized_sumnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "02d518ac1fc09ef5dc4eabe4d9fe87a26d09bc9551d9c7d27686a6ac8a51ace1"
  },
  "kernelspec": {
   "display_name": "Python 3.7.6 ('rak-env': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
